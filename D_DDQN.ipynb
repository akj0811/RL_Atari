{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D DDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQ9dC3A/KEBMqQBDll9aT5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akj0811/RL_Atari/blob/master/D_DDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55eLkAsD3Dfn",
        "colab_type": "code",
        "outputId": "8ff402c7-412a-48f6-cb27-7b126d6cb2f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# !apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import random\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQi9i7WJ87Zf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = [None] * max_size\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = random.sample(range(self.size), batch_size)\n",
        "        return [self.buffer[index] for index in indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue7soTY08-UF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "    def __init__(self, frame_size, num_actions):\n",
        "        self.frame_size = frame_size\n",
        "        self.num_actions = num_actions\n",
        "        self.model = self.build_network()\n",
        "    \n",
        "    def build_network(self):\n",
        "        input = keras.Input(shape = (84, 84, self.frame_size))\n",
        "        layer1 = keras.layers.Conv2D(32, kernel_size = (3, 3), activation = 'relu')(input)\n",
        "        max_pool = keras.layers.MaxPooling2D(pool_size = (2, 2))(layer1)\n",
        "        layer2 = keras.layers.Conv2D(32, kernel_size = (5, 5), activation = 'relu')(max_pool)\n",
        "        max_pool = keras.layers.MaxPooling2D(pool_size = (2, 2))(layer2)\n",
        "        flatten = keras.layers.Flatten()(max_pool)\n",
        "        advantage = keras.layers.Dense(self.num_actions, activation = 'relu')(flatten)\n",
        "        ad = keras.layers.Lambda(lambda x :tf.reduce_mean(x, axis = -1))(advantage)\n",
        "        ans = keras.layers.Add()([advantage, -ad])\n",
        "        state_value = keras.layers.Dense(1)(flatten)\n",
        "        output = keras.layers.Add()([advantage, state_value])\n",
        "\n",
        "        self.model = keras.Model(\n",
        "            inputs = [input],\n",
        "            outputs = [output]\n",
        "        )\n",
        "        self.model.compile(loss = 'mse', optimizer = 'adam', metrics = ['accuracy'])\n",
        "        return self.model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oVbXMJ99Aby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, agent_params):\n",
        "        self.discount = agent_params['discount']\n",
        "        self.epsilon = agent_params['epsilon']\n",
        "        self.num_actions = agent_params['num_actions']\n",
        "        self.exp_size = agent_params['exp_size']\n",
        "        self.frame_size = agent_params['frame_size']\n",
        "        self.merge = agent_params['merge_threshold']\n",
        "        self.batch_size = agent_params['batch_size']\n",
        "        self.last_action = None\n",
        "        self.last_state = None\n",
        "        self.exp = ReplayMemory(self.exp_size)\n",
        "        self.frames = np.zeros((1, 84, 84, self.frame_size))\n",
        "        self.target_model = Model(self.frame_size, self.num_actions)\n",
        "        self.value_model = Model(self.frame_size, self.num_actions)\n",
        "        self.num_updates = 0\n",
        "        self.num_steps = 0\n",
        "        self.num_games = 0\n",
        "        self.num_merge = 0\n",
        "\n",
        "    def phi(self, state):\n",
        "        new = self.frames[:, :, :, :self.frame_size - 1].copy()\n",
        "        new = np.concatenate([new, state/255.0], axis = -1)\n",
        "        self.frames = new.copy()\n",
        "        return self.frames\n",
        "\n",
        "    def run(self):\n",
        "        updates = min(self.batch_size, self.exp.size)\n",
        "        self.num_updates += updates\n",
        "        batch = self.exp.sample(updates)\n",
        "\n",
        "        input = [last_state for last_state, last_action, reward, state, terminal in batch]\n",
        "        inp = [state for last_state, last_action, reward, state, terminal in batch]\n",
        "        x_train = np.concatenate(input, axis = 0)\n",
        "        y_train = self.value_model.model.predict(x_train)\n",
        "        x_target = np.concatenate(inp, axis = 0)\n",
        "        y_target = self.target_model.model.predict(x_target)\n",
        "\n",
        "        for count, memory in enumerate(batch):\n",
        "            last_state, last_action, reward, state, terminal = memory\n",
        "            if terminal == 0:\n",
        "                y_train[count][last_action] = reward + self.discount*(y_target[count][np.argmax(y_train[count])])\n",
        "            else:\n",
        "                y_train[count][last_action] = reward\n",
        "            \n",
        "        self.value_model.model.fit(x_train, y_train, verbose = 0, epochs = 1)\n",
        "\n",
        "        if self.num_updates >= self.merge:\n",
        "            self.merge_model()\n",
        "\n",
        "    def merge_model(self):\n",
        "        self.target_model.model.set_weights(self.value_model.model.get_weights())\n",
        "        self.num_merge += 1\n",
        "        self.num_updates = 0\n",
        "\n",
        "    def epsilon_decay(self):\n",
        "        if self.epsilon > 0.1:\n",
        "            self.epsilon -= 0.9/1000000\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            action = np.argmax(self.value_model.model.predict(state)[0])\n",
        "    \n",
        "        self.num_steps += 1\n",
        "        self.epsilon_decay()\n",
        "        return action\n",
        "\n",
        "    def agent_start(self, env_state):\n",
        "        state = self.phi(env_state)\n",
        "        self.last_action = self.choose_action(state)\n",
        "        self.last_state = state\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_step(self, reward, env_state):\n",
        "        state = self.phi(env_state)\n",
        "        memory = (self.last_state, self.last_action, reward, state, 0)\n",
        "        self.exp.append(memory)\n",
        "        self.run()\n",
        "        self.last_action = self.choose_action(state)\n",
        "        self.last_state = state\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_end(self, reward):\n",
        "        memory = (self.last_state, self.last_action, reward, np.zeros((1, 84, 84, 4)), 1)\n",
        "        self.exp.append(memory)\n",
        "        self.run()\n",
        "        self.num_games += 1\n",
        "\n",
        "    def plot(self, states):\n",
        "        y = np.mean(np.max(self.value_model.model.predict(states), axis = -1))\n",
        "        return y\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbrUVX5hp5M1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(image):\n",
        "    image = cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n",
        "    image = cv.resize(image, (84, 110))\n",
        "    image = image[18:102, :]\n",
        "    image = image.reshape(1, 84, 84, 1)\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_W7l9DV9Afk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent_params = {\n",
        "    'discount' :0.99,\n",
        "    'epsilon' : 1.0,\n",
        "    'num_actions' : 6,\n",
        "    'exp_size': 100000,\n",
        "    'frame_size' : 4,\n",
        "    'merge_threshold' : 100000,\n",
        "    'batch_size' : 32,\n",
        "}\n",
        "agent = Agent(agent_params)\n",
        "episodes = []\n",
        "q_value = []\n",
        "avg_reward = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzcz-agf9AkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "splapvkv9HuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = np.random.choice(agent.exp.size, 10000)\n",
        "test_states = np.zeros((10000, 84, 84, 4))\n",
        "for count, index in enumerate(indices):\n",
        "    test_states[count, :] = agent.exp.buffer[index][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5B1XeRE9Hx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(agent, env):\n",
        "  \n",
        "    num_games = 20\n",
        "    total = 0\n",
        "    sum = 0\n",
        "    for _ in range(1, num_games+1):\n",
        "        action = agent.agent_start(pre_process(env.reset()))\n",
        "        observations, reward, done, info = env.step([action])\n",
        "        total += reward\n",
        "        sum += reward\n",
        "        while not done:\n",
        "            observations, reward, done, info = env.step([agent.agent_step(reward, pre_process(observations))])\n",
        "            total += reward\n",
        "\n",
        "        agent.agent_end(reward)\n",
        "        if agent.num_games % 50 == 0:\n",
        "            episodes.append(agent.num_games)\n",
        "            q_value.append(agent.plot(test_states))\n",
        "            avg_reward.append(total/50)\n",
        "            total = 0\n",
        "\n",
        "        if agent.num_games % 100 == 0:\n",
        "            print('Games = {}, Steps = {}, Reward = {}'.format(_, agent.num_steps, sum/100))\n",
        "            sum = 0\n",
        "\n",
        "        if agent.num_games % 400 == 0:\n",
        "            with open('/content/drive/My Drive/DDQN/episodes0.pkl', 'wb') as f:\n",
        "                pickle.dump(episodes, f)\n",
        "            with open('/content/drive/My Drive/DDQN/q_value0.pkl', 'wb') as f:\n",
        "                pickle.dump(q_value, f)\n",
        "            with open('/content/drive/My Drive/DDQN/avg_reward0.pkl', 'wb') as f:\n",
        "                pickle.dump(avg_reward, f)\n",
        "            with open('/content/drive/My Drive/DDQN/test_states0.pkl', 'wb') as f:\n",
        "                pickle.dump(test_states, f)\n",
        "            with open('/content/drive/My Drive/DDQN/agent0.pkl', 'wb') as f:\n",
        "                pickle.dump(agent, f)\n",
        "        elif agent.num_games % 200 == 0:\n",
        "            with open('/content/drive/My Drive/DDQN/episodes1.pkl', 'wb') as f:\n",
        "                pickle.dump(episodes, f)\n",
        "            with open('/content/drive/My Drive/DDQN/q_value1.pkl', 'wb') as f:\n",
        "                pickle.dump(q_value, f)\n",
        "            with open('/content/drive/My Drive/DDQN/avg_reward1.pkl', 'wb') as f:\n",
        "                pickle.dump(avg_reward, f)\n",
        "            with open('/content/drive/My Drive/DDQN/test_states1.pkl', 'wb') as f:\n",
        "                pickle.dump(test_states, f)\n",
        "            with open('/content/drive/My Drive/DDQN/agent1.pkl', 'wb') as f:\n",
        "                pickle.dump(agent, f)\n",
        "\n",
        "    plt.plot(episodes, q_value)\n",
        "    plt.plot(episodes, avg_reward)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuZ80ec99MjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(agent, env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeVj_JZs9PIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/DDQN/agent0.pkl', 'rb') as f:\n",
        "    agent = pickle.load(f)\n",
        "with open('/content/drive/My Drive/DDQN/episodes0.pkl', 'rb') as f:\n",
        "    episodes = pickle.load(f)\n",
        "with open('/content/drive/My Drive/DDQN/q_value0.pkl', 'rb') as f:\n",
        "    q_value = pickle.load(f)\n",
        "with open('/content/drive/My Drive/DDQN/avg_reward0.pkl', 'rb') as f:\n",
        "    avg_reward = pickle.load(f)\n",
        "with open('/content/drive/My Drive/DDQN/test_states0.pkl', 'rb') as f:\n",
        "    test_states = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr1Uxgzd9SXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(episodes, q_value)\n",
        "plt.plot(episodes, avg_reward)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlWxAHEcZUHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(agent.num_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}