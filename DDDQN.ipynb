{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1YpgOpnd9XFasCfvfwnZA-z7BfrOSAXLJ",
      "authorship_tag": "ABX9TyPcN8dBhajI0MhwvexMdun2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WM1LcDLKour"
      },
      "source": [
        "# !apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Dense, Add, Lambda\n",
        "from keras import Input\n",
        "from keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqoWWdZDLgy7"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = [None] * max_size\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = random.sample(range(self.size), batch_size)\n",
        "        return [self.buffer[index] for index in indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-s4TBYkL1cW"
      },
      "source": [
        "class Network:\n",
        "    def __init__(self, frame_size, num_actions):\n",
        "        inp = Input(shape = (frame_size*128, ))\n",
        "        dense1 = Dense(256, activation = 'relu')(inp)\n",
        "        dense2 = Dense(128, activation = 'relu')(dense1)\n",
        "        action1 = Dense(128, activation = 'relu')(dense2)\n",
        "        action = Dense(num_actions)(action1)\n",
        "        ad = Lambda(lambda x : -tf.reduce_mean(x, axis = -1, keepdims = True))(action)\n",
        "        ans = Add()([action, ad])\n",
        "\n",
        "        value1 = Dense(128, activation = 'relu')(dense2)\n",
        "        value = Dense(1)(value1)\n",
        "\n",
        "        output = Add()([ans, value])\n",
        "\n",
        "        model = Model(inp, output)\n",
        "        model.compile(optimizer = keras.optimizers.Adam(learning_rate = 0.00001), loss = 'mse', metrics = ['accuracy'])\n",
        "        self.model = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob5cTxsWWjXm"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, agent_params):\n",
        "        self.discount = agent_params['discount']\n",
        "        self.epsilon = agent_params['epsilon']\n",
        "        self.num_actions = agent_params['num_actions']\n",
        "        self.exp_size = agent_params['exp_size']\n",
        "        self.frame_size = agent_params['frame_size']\n",
        "        self.merge = agent_params['merge_threshold']\n",
        "        self.batch_size = agent_params['batch_size']\n",
        "        self.last_action = None\n",
        "        self.last_state = None\n",
        "        self.exp = ReplayMemory(self.exp_size)\n",
        "        self.frames = np.zeros((1, self.frame_size*128))\n",
        "        self.target_model = Network(self.frame_size, self.num_actions)\n",
        "        self.value_model = Network(self.frame_size, self.num_actions)\n",
        "        self.num_updates = 0\n",
        "        self.num_steps = 0\n",
        "        self.num_games = 0\n",
        "        self.num_merge = 0\n",
        "        self.waste = 0\n",
        "        self.lives = agent_params['lives']\n",
        "        self.target_model.model.set_weights(self.value_model.model.get_weights())\n",
        "  \n",
        "    def phi(self, state):\n",
        "        new = self.frames[:, 128:].copy()\n",
        "        new = np.concatenate([new, state/255.0], axis = -1)\n",
        "        self.frames = new.copy()\n",
        "        return self.frames\n",
        "    \n",
        "    def phi_init(self):\n",
        "        self.frames = np.zeros((1, self.frame_size*128))\n",
        "\n",
        "    def run(self):\n",
        "        updates = min(self.batch_size, self.exp.size)\n",
        "        self.num_updates += updates\n",
        "        batch = self.exp.sample(updates)\n",
        "\n",
        "        input = [x[0] for x in batch]\n",
        "        inp = [x[3] for x in batch]\n",
        "        x_train = np.concatenate(input, axis = 0)\n",
        "        y_train = self.value_model.model.predict(x_train)\n",
        "        target_action = np.argmax(y_train, axis = 1)\n",
        "        x_target = np.concatenate(inp, axis = 0)\n",
        "        y_target = self.target_model.model.predict(x_target)\n",
        "\n",
        "        for count, memory in enumerate(batch):\n",
        "            last_state, last_action, reward, state, terminal = memory\n",
        "            y_train[count][last_action] = reward + self.discount*y_target[count][target_action[count]]*(1 - terminal)\n",
        "\n",
        "        self.value_model.model.fit(x_train, y_train, verbose = 0, epochs = 1)\n",
        "\n",
        "        if self.num_steps % self.merge == 0:\n",
        "            self.merge_model()\n",
        "\n",
        "    def merge_model(self):\n",
        "        self.target_model.model.set_weights(self.value_model.model.get_weights())\n",
        "        self.num_merge += 1\n",
        "\n",
        "    def epsilon_decay(self):\n",
        "        if self.epsilon >= 0.05:\n",
        "            if self.num_steps >= 50000:\n",
        "                if self.num_steps <= 1050000:\n",
        "                    self.epsilon -= 0.9/1000000\n",
        "                elif self.num_steps <= 2050000:\n",
        "                    self.epsilon -= 0.05/1000000\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            action = np.argmax(self.value_model.model.predict(state)[0])\n",
        "\n",
        "        self.num_steps += 1\n",
        "        self.epsilon_decay()\n",
        "        return action\n",
        "\n",
        "    def agent_start(self, env_state):\n",
        "        self.lives = 5\n",
        "        self.phi_init()\n",
        "        state = self.phi(env_state)\n",
        "        self.last_action = self.choose_action(state)\n",
        "        self.last_state = state\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_step(self, reward, env_state, lives):\n",
        "        state = self.phi(env_state)\n",
        "        if self.waste:\n",
        "            self.waste = 0\n",
        "        elif self.lives > lives:\n",
        "            self.lives = lives\n",
        "            memory = (self.last_state, self.last_action, reward, state, 1)\n",
        "            self.waste = 1\n",
        "            self.phi_init()\n",
        "            self.exp.append(memory)\n",
        "        else:\n",
        "            memory = (self.last_state, self.last_action, reward, state, 0)\n",
        "            self.exp.append(memory)\n",
        "        self.run()\n",
        "        self.last_action = self.choose_action(state)\n",
        "        self.last_state = state\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_end(self, reward):\n",
        "        memory = (self.last_state, self.last_action, reward, np.zeros((1, 512)), 1)\n",
        "        self.exp.append(memory)\n",
        "        self.run()\n",
        "        self.num_games += 1\n",
        "\n",
        "    def check(self, states):\n",
        "        y = np.mean(np.max(self.target_model.model.predict(states), axis = 1))\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pqlFdOWWpKI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd3a9a03-0308-4f13-8bb2-b0ebd2c35d4f"
      },
      "source": [
        "agent_params = {\n",
        "    'discount' :0.99,\n",
        "    'epsilon' :1.0,\n",
        "    'num_actions' : 4,\n",
        "    'exp_size': 600000,\n",
        "    'frame_size' : 4,\n",
        "    'merge_threshold' : 10000,\n",
        "    'batch_size' : 32,\n",
        "    'lives' : 5\n",
        "}\n",
        "agent = Agent(agent_params)\n",
        "episodes = []\n",
        "q_value = []\n",
        "avg_reward = []\n",
        "\n",
        "env = gym.make('Breakout-ramDeterministic-v4')\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxnwGsNjWr23"
      },
      "source": [
        "indices = np.random.choice(agent.exp.size, 20000)\n",
        "test_states = np.zeros((20000, 512))\n",
        "for count, index in enumerate(indices):\n",
        "    test_states[count, :] = agent.exp.buffer[index][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqNhBlm60CNx"
      },
      "source": [
        "def save(num_games):  \n",
        "    val = 1\n",
        "    if num_games % 400 == 0:\n",
        "        val = 0\n",
        "    data = (agent.epsilon, agent.exp, agent.num_updates, agent.num_steps, agent.num_games, agent.num_merge)\n",
        "    with open('/content/drive/My Drive/DDDQN/parameters' + str(val) + '.pkl', 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "    data = (episodes, q_value, avg_reward, test_states)\n",
        "    with open('/content/drive/My Drive/DDDQN/auxiliary' + str(val) + '.pkl', 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "    agent.value_model.model.save('/content/drive/My Drive/DDDQN/value_model' + str(val) + '.h5')\n",
        "    agent.target_model.model.save('/content/drive/My Drive/DDDQN/target_model' + str(val) + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUBBAW3z3v5R"
      },
      "source": [
        "def load(val):\n",
        "    global agent, episodes, q_value, avg_reward, test_states\n",
        "    with open('/content/drive/My Drive/DDDQN/parameters' + str(val) + '.pkl', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    agent.epsilon, agent.exp, agent.num_updates, agent.num_steps, agent.num_games, agent.num_merge = data\n",
        "    with open('/content/drive/My Drive/DDDQN/auxiliary' + str(val) + '.pkl', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    episodes, q_value, avg_reward, test_states = data\n",
        "    agent.value_model.model = keras.models.load_model('/content/drive/My Drive/DDDQN/value_model' + str(val) + '.h5', custom_objects = {\"tf\" : tf})\n",
        "    agent.target_model.model = keras.models.load_model('/content/drive/My Drive/DDDQN/target_model' + str(val) + '.h5', custom_objects = {\"tf\" : tf})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeXeUCbm8AVV"
      },
      "source": [
        "def plot(num_games, total):\n",
        "    episodes.append(num_games)\n",
        "    q_value.append(agent.check(test_states))\n",
        "    avg_reward.append(total)\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8al01TECGQ2"
      },
      "source": [
        "def record(out):\n",
        "    image = env.unwrapped._get_image()\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    out.write(image)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IexpOxyYWtTv"
      },
      "source": [
        "def train(agent, env):\n",
        "    load(0)\n",
        "    num_games = 2000\n",
        "    total = 0\n",
        "    sum = 0\n",
        "    for _ in range(1, num_games+1):\n",
        "        # fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "        # out = cv2.VideoWriter('/content/drive/My Drive/output' + str(_) + '.avi',fourcc,20, (160, 210))\n",
        "        action = agent.agent_start(env.reset().reshape((1, 128)))\n",
        "        observations, reward, done, info = env.step([action])\n",
        "        # out = record(out)\n",
        "        total += reward\n",
        "        sum += reward\n",
        "\n",
        "        while not done:\n",
        "            observations, reward, done, info = env.step([agent.agent_step(reward, observations.reshape((1, 128)), env.ale.lives())])\n",
        "            # out = record(out)\n",
        "            total += reward\n",
        "            sum += reward\n",
        "        agent.agent_end(reward)\n",
        "\n",
        "        if agent.num_games % 50 == 0:\n",
        "            total = plot(agent.num_games, total/50)\n",
        "            if agent.num_games % 100 == 0:\n",
        "                print('Games = {}, Steps = {}, Reward = {}'.format(_, agent.num_steps, sum/100))\n",
        "                sum = 0\n",
        "                if agent.num_games % 200 == 0:\n",
        "                    save(agent.num_games)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKTrofsr-3IG"
      },
      "source": [
        "def evaluate():\n",
        "    print(agent.num_games)\n",
        "    print(agent.num_steps)\n",
        "    print(agent.epsilon)\n",
        "    print(np.max(agent.value_model.model.get_weights()[0]))\n",
        "    _, ax = plt.subplots(1, 2, figsize = (10, 5))\n",
        "    ax[0].plot(episodes, q_value)\n",
        "    ax[1].plot(episodes, avg_reward)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVVqAMXHWvoj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9c4c41bc-b1e6-4f23-b45f-7dfa6ca0175e"
      },
      "source": [
        "train(agent, env)\n",
        "evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Games = 100, Steps = 11934510, Reward = 68.77\n",
            "Games = 200, Steps = 12068046, Reward = 99.63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0Nm_iTvXNQJ"
      },
      "source": [
        "class Performer:\n",
        "    def __init__(self, performer_params):\n",
        "        self.num_actions = performer_params['num_actions']\n",
        "        self.frame_size = performer_params['frame_size']\n",
        "        self.last_action = None\n",
        "        self.frames = np.zeros((1, self.frame_size*128))\n",
        "        self.predict_model = Model(self.frame_size, self.num_actions)\n",
        "        self.num_steps = 0\n",
        "        self.lives = agent_params['lives']\n",
        "        self.epsilon = performer_params['epsilon']\n",
        "  \n",
        "    def phi(self, state):\n",
        "        new = self.frames[:, 128:].copy()\n",
        "        new = np.concatenate([new, state/255.0], axis = -1)\n",
        "        self.frames = new.copy()\n",
        "        return self.frames\n",
        "    \n",
        "    def phi_init(self):\n",
        "        self.frames = np.zeros((1, self.frame_size*128))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            action = np.argmax(self.predict_model.model.predict(state)[0])\n",
        "        self.num_steps += 1\n",
        "        return action\n",
        "\n",
        "    def agent_start(self, env_state):\n",
        "        self.lives = 5\n",
        "        self.phi_init()\n",
        "        state = self.phi(env_state)\n",
        "        self.last_action = self.choose_action(state)\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_step(self, reward, env_state, lives):\n",
        "        state = self.phi(env_state)\n",
        "        if self.lives > lives:\n",
        "            self.lives = lives\n",
        "            self.phi_init()\n",
        "        self.last_action = self.choose_action(state)\n",
        "        return self.last_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lliu6ay7XN2n"
      },
      "source": [
        "performer = Performer(performer_params = {'num_actions' : 4, 'frame_size'  : 4, 'lives' : 5, 'epsilon' : 0.05})\n",
        "performer.predict_model.model.set_weights(agent.target_model.model.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koJbsA9kXRFv"
      },
      "source": [
        "def perform(performer, env):\n",
        "    num_games = 20\n",
        "    for i in range(1, num_games+1):\n",
        "        total = 0\n",
        "        action = performer.agent_start(cv2_imshow(env.reset())env.reset().reshape((1, 128)))\n",
        "        observations, reward, done, info = env.step([action])\n",
        "        total += reward\n",
        "\n",
        "        while not done:\n",
        "            observations, reward, done, info = env.step([performer.agent_step(reward, observations.reshape((1, 128)), env.ale.lives())])\n",
        "            total += reward\n",
        "        print('Game {}, Score = {}, Steps = {}'.format(i, total, performer.num_steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltxvvKAwXSkt"
      },
      "source": [
        "perform(performer, env)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}