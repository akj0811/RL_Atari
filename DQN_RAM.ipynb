{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPA9qgkGv9UBxwA1CoqFrYp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akj0811/RL_Atari/blob/master/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyUwxfv95Xsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX0lKOT39AcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = [None] * max_size\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = random.sample(range(self.size), batch_size)\n",
        "        return [self.buffer[index] for index in indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYuIWy69Alyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "    def __init__(self, frame_size, num_actions):\n",
        "        self.model = keras.Sequential()\n",
        "        self.model.add(keras.layers.Dense(128, activation = 'relu', input_shape = (frame_size*128, )))\n",
        "        self.model.add(keras.layers.Dense(60, activation = 'relu'))\n",
        "        self.model.add(keras.layers.Dense(num_actions))\n",
        "        self.model.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKw3gyGK7H-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, agent_params):\n",
        "        self.discount = agent_params['discount']\n",
        "        self.epsilon = agent_params['epsilon']\n",
        "        self.num_actions = agent_params['num_actions']\n",
        "        self.exp_size = agent_params['exp_size']\n",
        "        self.frame_size = agent_params['frame_size']\n",
        "        self.merge = agent_params['merge_threshold']\n",
        "        self.batch_size = agent_params['batch_size']\n",
        "        self.last_action = None\n",
        "        self.last_state = None\n",
        "        self.exp = ReplayMemory(self.exp_size)\n",
        "        self.frames = np.zeros((1, self.frame_size*128))\n",
        "        self.target_model = Model(self.frame_size, self.num_actions)\n",
        "        self.value_model = Model(self.frame_size, self.num_actions)\n",
        "        self.num_updates = 0\n",
        "        self.num_steps = 0\n",
        "        self.num_games = 0\n",
        "        self.num_merge = 0\n",
        "\n",
        "    def phi(self, state):\n",
        "        new = self.frames[:, 128:].copy()\n",
        "        new = np.concatenate([new, state/255.0], axis = -1)\n",
        "        self.frames = new.copy()\n",
        "        return self.frames\n",
        "\n",
        "    def run(self):\n",
        "        updates = min(self.batch_size, self.exp.size)\n",
        "        self.num_updates += updates\n",
        "        batch = self.exp.sample(updates)\n",
        "\n",
        "        input = [last_state for last_state, last_action, reward, state, terminal in batch]\n",
        "        x_train = np.concatenate(input, axis = 0)\n",
        "        y_train = self.value_model.model.predict(x_train)\n",
        "\n",
        "        for count, memory in enumerate(batch):\n",
        "            last_state, last_action, reward, state, terminal = memory\n",
        "            y_train[count][last_action] = reward\n",
        "            if terminal == 0:\n",
        "                y_train[count][last_action] += self.discount*np.amax(self.target_model.model.predict(state))\n",
        "\n",
        "        self.value_model.model.fit(x_train, y_train, verbose = 0, epochs = 1)\n",
        "\n",
        "        if self.num_updates >= self.merge:\n",
        "            self.merge_model()\n",
        "\n",
        "    def merge_model(self):\n",
        "        self.target_model.model.set_weights(self.value_model.model.get_weights())\n",
        "        self.num_merge += 1\n",
        "        self.num_updates = 0\n",
        "\n",
        "    def epsilon_decay(self):\n",
        "        if self.epsilon > 0.1:\n",
        "            self.epsilon -= 0.9/1000000\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            action = np.argmax(self.value_model.model.predict(state)[0])\n",
        "    \n",
        "        self.num_steps += 1\n",
        "        self.epsilon_decay()\n",
        "        return action\n",
        "\n",
        "    def agent_start(self, env_state):\n",
        "        state = self.phi(env_state)\n",
        "        self.last_action = self.choose_action(state)\n",
        "        self.last_state = state\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_step(self, reward, env_state):\n",
        "        state = self.phi(env_state)\n",
        "        memory = (self.last_state, self.last_action, reward, state, 0)\n",
        "        self.exp.append(memory)\n",
        "        self.run()\n",
        "        self.last_action = self.choose_action(state)\n",
        "        self.last_state = state\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_end(self, reward):\n",
        "        memory = (self.last_state, self.last_action, reward, 0, 1)\n",
        "        self.exp.append(memory)\n",
        "        self.run()\n",
        "        self.num_games += 1\n",
        "\n",
        "    def plot(self, states):\n",
        "        y = np.mean(self.value_model.model.predict(states))\n",
        "        return y\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tkITD-4AJ75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent_params = {\n",
        "    'discount' :0.99,\n",
        "    'epsilon' : 1.0,\n",
        "    'num_actions' : 4,\n",
        "    'exp_size': 1000000,\n",
        "    'frame_size' : 4,\n",
        "    'merge_threshold' : 100000,\n",
        "    'batch_size' : 32,\n",
        "}\n",
        "agent = Agent(agent_params)\n",
        "episodes = []\n",
        "q_value = []\n",
        "avg_reward = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlJ647mKAYbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('Breakout-ram-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jmlOB1lrg60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = np.random.choice(agent.exp.size, 20000)\n",
        "test_states = np.zeros((20000, 512))\n",
        "for count, index in enumerate(indices):\n",
        "    test_states[count, :] = agent.exp.buffer[index][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcM_jNU3JLR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(agent, env):\n",
        "  \n",
        "    num_games = 100\n",
        "    total = 0\n",
        "    for _ in range(1, num_games+1):\n",
        "        action = agent.agent_start(env.reset().reshape((1, 128)))\n",
        "        observations, reward, done, info = env.step([action])\n",
        "        total += reward\n",
        "\n",
        "        while not done:\n",
        "            observations, reward, done, info = env.step([agent.agent_step(reward, observations.reshape((1, 128)))])\n",
        "            total += reward\n",
        "\n",
        "        agent.agent_end(reward)\n",
        "\n",
        "        if agent.num_games % 20 == 0:\n",
        "            print('Games = {}, Steps = {}, Reward = {}'.format(_, agent.num_steps, total/20))\n",
        "            episodes.append(agent.num_games)\n",
        "            q_value.append(agent.plot(test_states))\n",
        "            avg_reward.append(total/20)\n",
        "            total = 0\n",
        "\n",
        "        if agent.num_games % 400 == 0:\n",
        "            with open('/content/drive/My Drive/agent0.pkl', 'wb') as f:\n",
        "                pickle.dump(agent, f)\n",
        "            with open('/content/drive/My Drive/episodes0.pkl', 'wb') as f:\n",
        "                pickle.dump(episodes, f)\n",
        "            with open('/content/drive/My Drive/q_value0.pkl', 'wb') as f:\n",
        "                pickle.dump(q_value, f)\n",
        "            with open('/content/drive/My Drive/avg_reward0.pkl', 'wb') as f:\n",
        "                pickle.dump(avg_reward, f)\n",
        "            with open('/content/drive/My Drive/test_states0.pkl', 'wb') as f:\n",
        "                pickle.dump(test_states, f)\n",
        "        elif agent.num_games % 200 == 0:\n",
        "            with open('/content/drive/My Drive/agent2.pkl', 'wb') as f:\n",
        "                pickle.dump(agent, f)\n",
        "            with open('/content/drive/My Drive/episodes2.pkl', 'wb') as f:\n",
        "                pickle.dump(episodes, f)\n",
        "            with open('/content/drive/My Drive/q_value2.pkl', 'wb') as f:\n",
        "                pickle.dump(q_value, f)\n",
        "            with open('/content/drive/My Drive/avg_reward2.pkl', 'wb') as f:\n",
        "                pickle.dump(avg_reward, f)\n",
        "            with open('/content/drive/My Drive/test_states2.pkl', 'wb') as f:\n",
        "                pickle.dump(test_states, f)\n",
        "\n",
        "    plt.plot(episodes, q_value)\n",
        "    plt.plot(episodes, avg_reward)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAHY6h7EPMoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(agent, env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dhon0iOwEaNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/agent0.pkl', 'rb') as f:\n",
        "    agent = pickle.load(f)\n",
        "with open('/content/drive/My Drive/episodes0.pkl', 'rb') as f:\n",
        "    episodes = pickle.load(f)\n",
        "with open('/content/drive/My Drive/q_value0.pkl', 'rb') as f:\n",
        "    q_value = pickle.load(f)\n",
        "with open('/content/drive/My Drive/avg_reward0.pkl', 'rb') as f:\n",
        "    avg_reward = pickle.load(f)\n",
        "with open('/content/drive/My Drive/test_states0.pkl', 'rb') as f:\n",
        "    test_states = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44SAmLd0zJ11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(episodes, q_value)\n",
        "plt.plot(episodes, avg_reward)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
